# Data Science Concepts
- [Chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test) A chi-squared test, also written as χ2 test, is any statistical hypothesis test where the sampling distribution of the test statistic is a chi-squared distribution when the null hypothesis is true.
- [p-value](https://en.wikipedia.org/wiki/P-value)
- [Degrees of Freedom](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics))
- [Interquartile range](https://en.wikipedia.org/wiki/Interquartile_range) Building block of box plot.
- [Normal probability plot](https://en.wikipedia.org/wiki/Normal_probability_plot)
he normal probability plot is a graphical technique to identify substantive 
departures from normality. This includes identifying outliers, skewness, kurtosis, 
a need for transformations, and mixtures. Normal probability plots are made of raw data, r
esiduals from model fits, and estimated parameters.
- [Boltzman Machine](https://en.wikipedia.org/wiki/Boltzmann_machine) A Boltzmann machine
(also called stochastic Hopfield network with hidden units) is a type of stochastic 
recurrent neural network (and Markov random field).
- [Markov random field](https://en.wikipedia.org/wiki/Markov_random_field)r undirected 
  graphical model is a set of random variables having a Markov property described by an undirected graph.
- [Restrcted Boltzmann Machine](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine)
A restricted Boltzmann machine (RBM) is a generative stochastic artificial 
neural network that can learn a probability distribution over its set of inputs.
  - . RBMs have found applications in dimensionality reduction, classification, 
  collaborative filtering, feature learning and topic modelling.
  They can be trained in either supervised or unsupervised ways, depending on the task.
-[Expectation Maximization with Gaussian Mixture Models](http://www.aishack.in/tutorials/expectation-maximization-gaussian-mixture-model/)
how to model multivariate data with a Gaussian Mixture Model. 
For training this model, we use a technique called Expectation Maximization.
- [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy)
  -Related to [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
## Bayesian
- [Bayesian_inference](https://en.wikipedia.org/wiki/Bayesian_inference)
- [Conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior)

## Hypothesis testing
- [P-Value by StatQuest](https://www.youtube.com/watch?v=5Z9OIYA8He8)

## Categorical variables
- [Chi Squared example](https://www.youtube.com/watch?v=1Ldl5Zfcm1Y)
  - [Chi-squared test on wikipedia](https://en.wikipedia.org/wiki/Chi-squared_test)

## Regression
- [Multiple Regression](https://people.richland.edu/james/ictcm/2004/multiple.html)  <- Fav, simple and clear
- [Regression Analysis ala Stata](https://stats.idre.ucla.edu/stata/output/regression-analysis-2/) 
- [Regression Tutorial with Analysis Examples, by Jim](http://statisticsbyjim.com/regression/regression-tutorial-analysis-examples/)
- [Reading a regresion table](http://svmiller.com/blog/2014/08/reading-a-regression-table-a-guide-for-students/)
- [Princeton Regression Intro](https://dss.princeton.edu/online_help/analysis/regression_intro.htm)
- [Princeton Interpreting regression output](https://dss.princeton.edu/online_help/analysis/interpreting_regression.htm)
- [Statsmodel documentation](https://www.statsmodels.org/stable/index.html)
- [Stats Models vs SKLearn for Linear Regression](https://becominghuman.ai/stats-models-vs-sklearn-for-linear-regression-f19df95ad99b)
- [The Simple Linear Regression Model at Penn State](https://newonlinecourses.science.psu.edu/stat501/node/253/)
- [Light Gradient Boosting](https://lightgbm.readthedocs.io/en/latest/)

## Random Forest
- [Hyperparameter Tuning the Random Forest in Python](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)

## PCA
- [Tutorial on Principal Component Analysis](https://arxiv.org/pdf/1404.1100.pdf)
- [PCA on Quora](https://www.quora.com/What-is-the-difference-between-PCA-and-SVD)
- [PCA For categorical features](https://stackoverflow.com/questions/40795141/pca-for-categorical-features)
- [Multiple correspondence analysis](https://en.wikipedia.org/wiki/Multiple_correspondence_analysis)
In statistics, multiple correspondence analysis (MCA) is a data analysis technique 
for nominal categorical data, used to detect and represent underlying structures 
in a data set. It does this by representing data as points in a low-dimensional Euclidean space. 
- [What is the intuitive relationship between SVD 
  and PCA?](https://math.stackexchange.com/questions/3869/what-is-the-intuitive-relationship-between-svd-and-pca)

## Data simulation
- [Simulate Regression Model](https://blogs.sas.com/content/iml/2017/01/25/simulate-regression-model-sas.html)
- [Generating correlated random variables](https://www.youtube.com/watch?v=QCqsJVS8p5A)
- [Generating Multivariate Gaussian Random Numbers](http://www.aishack.in/tutorials/generating-multivariate-gaussian-random/)
- [Synthetic cardiovascular data](https://laderast.github.io/cvdRiskData/)

## Graph theory
- [A-gentle-introduction-to-graph-theory](https://medium.com/basecs/a-gentle-introduction-to-graph-theory-77969829ead8)
- [https://becominghuman.ai/to-all-data-scientists-the-one-graph-algorithm-you-need-to-know-59178dbb1ec2](https://becominghuman.ai/to-all-data-scientists-the-one-graph-algorithm-you-need-to-know-59178dbb1ec2)
Connected Components: you can think of Connected Components in very layman’s terms as sort of a hard clustering algorithm 
which finds clusters/islands in related/connected data.
- [Connected Components in MapReduce and Beyond](https://ai.google/research/pubs/pub43122)
  
## Platforms
- [neural-net-from-scratch-with-pytorch](https://medium.com/@tomgrek/building-your-first-neural-net-from-scratch-with-pytorch-56b0e9c84d54)

## Packages
-[Introduction to the Syuzhet Package](https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html)
The package comes with four sentiment dictionaries and provides a method for accessing 
the robust, but computationally expensive, sentiment extraction tool developed in the NLP group at Stanford.

 -[Stanford's CoreNLP](https://stanfordnlp.github.io/CoreNLP/)
 
## Datasets
- [lightgbm-baseline pet adoption](https://www.kaggle.com/peterhurford/pets-lightgbm-baseline-with-all-the-data)
  good kernel example
- [How much did it rain?](https://www.kaggle.com/c/how-much-did-it-rain/data)
- [Just Do it Tweets](https://www.kaggle.com/kappa123/exploring-justdoit-tweets)
- [Measures of Human Mobility Using Mobile Phone Records Enhanced with GIS Data](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0133630)
- [human+activity+recognition+using+smartphones](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones)

## Papers
- [A survey of results on mobile phone datasets analysis](https://arxiv.org/pdf/1502.03406.pdf)
- [Vincent D. Blondel on Arxiv](https://arxiv.org/search/physics?searchtype=author&query=Blondel%2C+V+D)
  - Includes: [Clean up or mess up: 
    thD4D-Senegal: The Second Mobile Phone Data for Development 
    Challengee effect of sampling biases on measurements of degree distributions 
    in mobile phone datasets](https://arxiv.org/abs/1609.09413), 
    [Markov modeling of online inter-arrival times](https://arxiv.org/abs/1509.04857),
    [Sensitivity analysis of a branching process evolving on 
    a network with application in epidemiology](https://arxiv.org/abs/1509.01860),
    [Estimating Food Consumption and Poverty Indices with Mobile Phone Data](https://arxiv.org/abs/1412.2595),
    [Data for Development: the D4D Challenge on Mobile Phone Data](https://arxiv.org/abs/1210.0137)
- [Structural Bayesian Linear Regression for
Hidden Markov Models](https://www.merl.com/publications/docs/TR2013-071.pdf)
- [A Linear Regression and Markov Chain Model
For the Arabian Horse Registry](https://apps.dtic.mil/dtic/tr/fulltext/u2/a267097.pdf)

## Neural Networks
- [A visual proof that neural nets can compute any function](http://neuralnetworksanddeeplearning.com/chap4.html)

## Misc
[Latent Variable Models (lava)](https://github.com/kkholst/lava)
-[CMS at LHC](http://opendata.cern.ch/docs/observing-higgs-over-one-petabyte-new-cms-open-data)
The CMS Collaboration at CERN is pleased to announce the release of the third batch of high-level 
open data from the CMS detector at the Large Hadron Collider (LHC), available on the 
- [CERN Open Data portal](http://opendata.cern.ch/)
- [Toolkit for Multivariate Data Analysis with ROOT](https://root.cern.ch/tmva)
  - [TMVA Users Guide](https://root.cern.ch/download/doc/tmva/TMVAUsersGuide.pdf)
  - [TMVA Summary](https://root.cern.ch/tmva/summary)
  
- [ML cheat sheet](https://ml-cheatsheet.readthedocs.io/en/latest/index.html)
